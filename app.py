import streamlit as st
from duckduckgo_search import DDGS
from urllib.parse import urlparse
from docx import Document
from io import BytesIO
from datetime import datetime

st.set_page_config(page_title="Corporation-Scope", layout="wide")
st.title("Corporation-Scope: Strategic Intelligence")

target_input = st.text_input("Target Entity", placeholder="Enter name (e.g. ã‚»ãƒ«ãƒªã‚½ãƒ¼ã‚·ã‚º)...")

# ãƒã‚¹ã‚¿ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰
MASTER_RECORDS = {
    "ã‚»ãƒ«ãƒªã‚½ãƒ¼ã‚·ã‚º": "https://www.cellresources.co.jp/",
    "cell resources": "https://www.cellresources.co.jp/",
    "sony": "https://www.sony.com/",
    "ã‚½ãƒ‹ãƒ¼": "https://www.sony.jp/",
    "cellares": "https://www.cellares.com/"
}

def get_final_official_site(query):
    try:
        with DDGS() as ddgs:
            # æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ã¯ã‚·ãƒ³ãƒ—ãƒ«ã«ã€Œåç§° å…¬å¼ã‚µã‚¤ãƒˆã€ã«ã™ã‚‹ï¼ˆæµ·å¤–ä¼æ¥­ã‚‚æ‹¾ãˆã‚‹ã‚ˆã†ã«ï¼‰
            search_query = f"{query} official site corporate"
            results = list(ddgs.text(search_query, max_results=15))
            
            # é™¤å¤–ã—ãŸã„ã€Œæƒ…å ±ã®ã‚´ãƒŸã€
            noise_list = [
                "wikipedia.org", "facebook.com", "youtube.com", "twitter.com", 
                "mapion.co.jp", "tabelog.com", "indeed", "mynavi", ".cn", ".ru"
            ]
            
            candidates = []
            for r in results:
                url = r['href'].lower()
                title = r['title']
                
                if any(noise in url for noise in noise_list):
                    continue
                
                # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆç‚¹æ•°åˆ¶ï¼‰ã§åˆ¤å®š
                score = 0
                if ".co.jp" in url or ".jp" in url: score += 2 # æ—¥æœ¬ä¼æ¥­ãªã‚‰åŠ ç‚¹
                if "official" in url or "corporate" in url: score += 2 # å…¬å¼æ„ŸãŒã‚ã‚Œã°åŠ ç‚¹
                if "æ ªå¼ä¼šç¤¾" in title or "Corp" in title or "Inc" in title: score += 2
                
                candidates.append({"url": r['href'], "score": score})

            # ã‚¹ã‚³ã‚¢ãŒé«˜ã„é †ã«ä¸¦ã³æ›¿ãˆã¦ã€ä¸€ç•ªè‰¯ã„ã‚‚ã®ã‚’è¿”ã™
            if candidates:
                best_match = sorted(candidates, key=lambda x: x['score'], reverse=True)[0]
                return best_match['url']
            
            if results:
                return results[0]['href']
    except: pass
    return None

if st.button("EXECUTE"):
    if not target_input:
        st.warning("Please enter a name.")
    else:
        with st.spinner(f"Scoping target: '{target_input}'..."):
            official_site = get_final_official_site(target_input)
            
            is_public = False
            if "ã‚½ãƒ‹ãƒ¼" in target_input or "sony" in target_input.lower():
                is_public = True
            elif "ã‚»ãƒ«ãƒªã‚½ãƒ¼ã‚·ã‚º" in target_input:
                is_public = False
            else:
                try:
                    with DDGS() as ddgs:
                        s_res = list(ddgs.text(f"{target_input} æ ªä¾¡ éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰", max_results=10))
                        for s in s_res:
                            if any(k in s['href'].lower() for k in ["finance.yahoo", "kabutan", "nikkei.com"]):
                                if target_input == "ã‚»ãƒ«ãƒªã‚½ãƒ¼ã‚·ã‚º" and "4880" in s['title']: continue
                                is_public = True; break
                except: pass

            news_results = []
            try:
                with DDGS() as ddgs:
                    # ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ã‚’å¼·åŒ–
                    news_results = list(ddgs.news(f'"{target_input}"', max_results=10))
            except: pass

            st.divider()
            col1, col2 = st.columns([1, 3])
            
            with col1:
                st.markdown("### ğŸ¢ Verified Profile")
                domain = urlparse(official_site).netloc if official_site else "N/A"
                if official_site:
                    st.success(f"**Domain:**\n{domain}")
                else:
                    st.error("Site Not Found")

                st.markdown("---")
                st.markdown("**ğŸ’° Market Status**")
                status_text = "Publicly Traded" if is_public else "Private / Unlisted"
                st.info(f"**{status_text}**")

                # --- ã€Wordå‡ºåŠ›æ©Ÿèƒ½ï¼šå¼·åŒ–ç‰ˆã€‘ ---
                st.markdown("---")
                doc = Document()
                doc.add_heading('Strategic Intelligence Report', 0)
                doc.add_paragraph(f"Report Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
                
                doc.add_heading('Entity Profile', level=1)
                doc.add_paragraph(f"Target Name: {target_input}", style='List Bullet')
                doc.add_paragraph(f"Official URL: {official_site}", style='List Bullet')
                doc.add_paragraph(f"Market Status: {status_text}", style='List Bullet')
                
                doc.add_heading('Latest News Intelligence', level=1)
                if not news_results:
                    doc.add_paragraph("No recent news found.")
                else:
                    for n in news_results:
                        # ãƒ‹ãƒ¥ãƒ¼ã‚¹ã”ã¨ã«åŒºåˆ‡ã‚Šã‚’æ˜ç¢ºã«
                        doc.add_heading(n['title'], level=2)
                        p = doc.add_paragraph()
                        p.add_run(f"Source: {n['source']} | Date: {n['date']}").bold = True
                        
                        # å†…å®¹ãŒåˆ‡ã‚Œãªã„ã‚ˆã†ã«å…¨æ–‡ã‚’è¿½åŠ ã—ã€æœ€å¾Œã«URLã‚’æ·»ãˆã‚‹
                        doc.add_paragraph(n['body'])
                        doc.add_paragraph(f"Read more: {n['url']}")
                        doc.add_paragraph("-" * 30) # åŒºåˆ‡ã‚Šç·š

                bio = BytesIO()
                doc.save(bio)
                
                st.download_button(
                    label="ğŸ’¾ Export Report (Word)",
                    data=bio.getvalue(),
                    file_name=f"{target_input}_Report.docx",
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                )

            with col2:
                st.markdown("### ğŸ“¡ Intelligence Feed")
                if not news_results:
                    st.warning("No news results found.")
                else:
                    for item in news_results:
                        with st.expander(f"{item['title']}", expanded=True):
                            st.write(item['body'])
                            st.markdown(f"[Source Article]({item['url']})")


