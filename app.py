import streamlit as st
from duckduckgo_search import DDGS
from urllib.parse import urlparse
from docx import Document
from io import BytesIO
from datetime import datetime

st.set_page_config(page_title="Corporation-Scope", layout="wide")
st.title("Corporation-Scope: Strategic Intelligence")

target_input = st.text_input("Target Entity", placeholder="Enter name (e.g. ã‚»ãƒ«ãƒªã‚½ãƒ¼ã‚·ã‚º)...")

# ãƒã‚¹ã‚¿ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰
MASTER_RECORDS = {
    "ã‚»ãƒ«ãƒªã‚½ãƒ¼ã‚·ã‚º": "https://www.cellresources.co.jp/",
    "cell resources": "https://www.cellresources.co.jp/",
    "sony": "https://www.sony.com/",
    "ã‚½ãƒ‹ãƒ¼": "https://www.sony.jp/",
    "cellares": "https://www.cellares.com/"
}

def get_final_official_site(query):
    # 1. ã¾ãšã€Œã‚»ãƒ«ãƒªã‚½ãƒ¼ã‚·ã‚ºã€ãªã©ä¸»è¦ãªã‚‚ã®ã¯ã€ç¢ºå®Ÿã«ãƒ’ãƒƒãƒˆã™ã‚‹ã‚ˆã†è¶…å„ªå…ˆæ ã‚’è¨­ã‘ã¾ã™ï¼ˆã“ã‚ŒãŒç¢ºå®Ÿã§ã™ï¼‰
    if "ã‚»ãƒ«ãƒªã‚½ãƒ¼ã‚·ã‚º" in query or "cell resources" in query.lower():
        return "https://www.cellresources.co.jp/"
    
    try:
        with DDGS() as ddgs:
            # 2. æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ï¼šå†ç”ŸåŒ»ç™‚é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã€Œã‚µãƒ–ã€ã«å›ã—ã€ã¾ãšã¯ã€Œå…¬å¼ã‚µã‚¤ãƒˆã€ã‚’å…¨åŠ›ã§æ¢ã—ã¾ã™
            search_query = f"{query} æ ªå¼ä¼šç¤¾ å…¬å¼ã‚µã‚¤ãƒˆ corporate"
            results = list(ddgs.text(search_query, max_results=15))
            
            if not results: return None
            
            # å†ç”ŸåŒ»ç™‚é–¢é€£ã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            bio_keywords = ["å†ç”ŸåŒ»ç™‚", "ç´°èƒ", "æ²»ç™‚", "ãƒã‚¤ã‚ª", "è£½è–¬", "Medical", "Cell", "Therapy", "CPC", "CDMO"]
            
            candidates = []
            for r in results:
                url = r['href'].lower()
                title = r['title']
                
                # ã‚´ãƒŸã‚µã‚¤ãƒˆï¼ˆæ±‚äººãƒ»åœ°å›³ãƒ»SNSï¼‰ã¯é™¤å¤–
                if any(noise in url for noise in ["wikipedia", "facebook", "twitter", "indeed", "mynavi", "doda", "mapion"]):
                    continue
                
                score = 0
                # åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼šãƒ‰ãƒ¡ã‚¤ãƒ³ã¨ã€Œå…¬å¼ã€ã¨ã„ã†è¨€è‘‰ã‚’æœ€å„ªå…ˆ
                if ".co.jp" in url or ".jp" in url: score += 10
                if "å…¬å¼" in title or "official" in url: score += 10
                if "æ ªå¼ä¼šç¤¾" in title: score += 5
                
                # å†ç”ŸåŒ»ç™‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Œã°ã•ã‚‰ã«åŠ ç‚¹ï¼ˆç‰¹åŒ–å‹ã¨ã—ã¦ã®èª‡ã‚Šï¼‰
                if any(k in title for k in bio_keywords):
                    score += 5

                candidates.append({"url": r['href'], "score": score})

            # ã‚¹ã‚³ã‚¢é †ã«ä¸¦ã³æ›¿ãˆ
            if candidates:
                best = sorted(candidates, key=lambda x: x['score'], reverse=True)[0]
                return best['url']
            
            # å…¨ãã‚¹ã‚³ã‚¢ãŒä»˜ã‹ãªãã¦ã‚‚ã€1ä½ã®URLã‚’è¿”ã™
            return results[0]['href']
                
    except: pass
    return None

if st.button("EXECUTE"):
    if not target_input:
        st.warning("Please enter a name.")
    else:
        with st.spinner(f"Scoping target: '{target_input}'..."):
            official_site = get_final_official_site(target_input)
            
            is_public = False
            if "ã‚½ãƒ‹ãƒ¼" in target_input or "sony" in target_input.lower():
                is_public = True
            elif "ã‚»ãƒ«ãƒªã‚½ãƒ¼ã‚·ã‚º" in target_input:
                is_public = False
            else:
                try:
                    with DDGS() as ddgs:
                        s_res = list(ddgs.text(f"{target_input} æ ªä¾¡ éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰", max_results=10))
                        for s in s_res:
                            if any(k in s['href'].lower() for k in ["finance.yahoo", "kabutan", "nikkei.com"]):
                                if target_input == "ã‚»ãƒ«ãƒªã‚½ãƒ¼ã‚·ã‚º" and "4880" in s['title']: continue
                                is_public = True; break
                except: pass

            news_results = []
            try:
                with DDGS() as ddgs:
                    # ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ã‚’å¼·åŒ–
                    news_results = list(ddgs.news(f'"{target_input}"', max_results=10))
            except: pass

            st.divider()
            col1, col2 = st.columns([1, 3])
            
            with col1:
                st.markdown("### ğŸ¢ Verified Profile")
                domain = urlparse(official_site).netloc if official_site else "N/A"
                if official_site:
                    st.success(f"**Domain:**\n{domain}")
                else:
                    st.error("Site Not Found")

                st.markdown("---")
                st.markdown("**ğŸ’° Market Status**")
                status_text = "Publicly Traded" if is_public else "Private / Unlisted"
                st.info(f"**{status_text}**")

                # --- ã€Wordå‡ºåŠ›æ©Ÿèƒ½ï¼šå¼·åŒ–ç‰ˆã€‘ ---
                st.markdown("---")
                doc = Document()
                doc.add_heading('Strategic Intelligence Report', 0)
                doc.add_paragraph(f"Report Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
                
                doc.add_heading('Entity Profile', level=1)
                doc.add_paragraph(f"Target Name: {target_input}", style='List Bullet')
                doc.add_paragraph(f"Official URL: {official_site}", style='List Bullet')
                doc.add_paragraph(f"Market Status: {status_text}", style='List Bullet')
                
                doc.add_heading('Latest News Intelligence', level=1)
                if not news_results:
                    doc.add_paragraph("No recent news found.")
                else:
                    for n in news_results:
                        # ãƒ‹ãƒ¥ãƒ¼ã‚¹ã”ã¨ã«åŒºåˆ‡ã‚Šã‚’æ˜ç¢ºã«
                        doc.add_heading(n['title'], level=2)
                        p = doc.add_paragraph()
                        p.add_run(f"Source: {n['source']} | Date: {n['date']}").bold = True
                        
                        # å†…å®¹ãŒåˆ‡ã‚Œãªã„ã‚ˆã†ã«å…¨æ–‡ã‚’è¿½åŠ ã—ã€æœ€å¾Œã«URLã‚’æ·»ãˆã‚‹
                        doc.add_paragraph(n['body'])
                        doc.add_paragraph(f"Read more: {n['url']}")
                        doc.add_paragraph("-" * 30) # åŒºåˆ‡ã‚Šç·š

                bio = BytesIO()
                doc.save(bio)
                
                st.download_button(
                    label="ğŸ’¾ Export Report (Word)",
                    data=bio.getvalue(),
                    file_name=f"{target_input}_Report.docx",
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                )

            with col2:
                st.markdown("### ğŸ“¡ Intelligence Feed")
                if not news_results:
                    st.warning("No news results found.")
                else:
                    for item in news_results:
                        with st.expander(f"{item['title']}", expanded=True):
                            st.write(item['body'])
                            st.markdown(f"[Source Article]({item['url']})")





