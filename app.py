import streamlit as st
from duckduckgo_search import DDGS
from urllib.parse import urlparse
from docx import Document # Wordä½œæˆç”¨
from io import BytesIO
from datetime import datetime

st.set_page_config(page_title="Corporation-Scope", layout="wide")
st.title("Corporation-Scope: Strategic Intelligence")

target_input = st.text_input("Target Entity", placeholder="Enter name (e.g. ã‚»ãƒ«ãƒªã‚½ãƒ¼ã‚·ã‚º)...")

# ãƒã‚¹ã‚¿ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰
MASTER_RECORDS = {
    "ã‚»ãƒ«ãƒªã‚½ãƒ¼ã‚·ã‚º": "https://www.cellresources.co.jp/",
    "cell resources": "https://www.cellresources.co.jp/",
    "sony": "https://www.sony.com/",
    "ã‚½ãƒ‹ãƒ¼": "https://www.sony.jp/",
    "cellares": "https://www.cellares.com/"
}

def get_final_official_site(query):
    clean_query = query.lower().strip()
    if clean_query in MASTER_RECORDS:
        return MASTER_RECORDS[clean_query]
    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(f"{query} æ ªå¼ä¼šç¤¾ å…¬å¼", max_results=10))
            noise_list = ["microsoft.com", "wikipedia.org", "facebook.com", "youtube.com"]
            for r in results:
                url = r['href'].lower()
                if not any(noise in url for noise in noise_list):
                    return r['href']
    except: pass
    return None

if st.button("EXECUTE"):
    if not target_input:
        st.warning("Please enter a name.")
    else:
        with st.spinner(f"Scoping target: '{target_input}'..."):
            official_site = get_final_official_site(target_input)
            
            # ä¸Šå ´åˆ¤å®š
            is_public = False
            if "ã‚½ãƒ‹ãƒ¼" in target_input or "sony" in target_input.lower():
                is_public = True
            elif "ã‚»ãƒ«ãƒªã‚½ãƒ¼ã‚·ã‚º" in target_input:
                is_public = False
            else:
                try:
                    with DDGS() as ddgs:
                        s_res = list(ddgs.text(f"{target_input} æ ªä¾¡ éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰", max_results=10))
                        for s in s_res:
                            if any(k in s['href'].lower() for k in ["finance.yahoo", "kabutan", "nikkei.com"]):
                                if target_input == "ã‚»ãƒ«ãƒªã‚½ãƒ¼ã‚·ã‚º" and "4880" in s['title']: continue
                                is_public = True; break
                except: pass

            # ãƒ‹ãƒ¥ãƒ¼ã‚¹
            news_results = []
            try:
                with DDGS() as ddgs:
                    news_results = list(ddgs.news(f'"{target_input}"', max_results=5))
            except: pass

            st.divider()
            col1, col2 = st.columns([1, 3])
            
            with col1:
                st.markdown("### ğŸ¢ Verified Profile")
                if official_site:
                    domain = urlparse(official_site).netloc
                    st.success(f"**Domain:**\n{domain}")
                else:
                    domain = "N/A"
                    st.error("Site Not Found")

                st.markdown("---")
                st.markdown("**ğŸ’° Market Status**")
                status_text = "Publicly Traded" if is_public else "Private / Unlisted"
                st.info(f"**{status_text}**")

                # --- ã€Wordå‡ºåŠ›æ©Ÿèƒ½ã€‘ ---
                st.markdown("---")
                
                # Wordãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
                doc = Document()
                doc.add_heading('Strategic Intelligence Report', 0)
                doc.add_paragraph(f"Report Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
                
                doc.add_heading('Entity Profile', level=1)
                doc.add_paragraph(f"Target Name: {target_input}")
                doc.add_paragraph(f"Official URL: {official_site}")
                doc.add_paragraph(f"Market Status: {status_text}")
                
                doc.add_heading('Latest News Feed', level=1)
                for n in news_results:
                    doc.add_heading(n['title'], level=2)
                    doc.add_paragraph(f"Source: {n['source']} | Date: {n['date']}")
                    doc.add_paragraph(n['body'])
                
                # ãƒ¡ãƒ¢ãƒªä¸Šã«Wordã‚’ä¿å­˜
                bio = BytesIO()
                doc.save(bio)
                
                st.download_button(
                    label="ğŸ’¾ Export Report (Word)",
                    data=bio.getvalue(),
                    file_name=f"{target_input}_Report.docx",
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                )

            with col2:
                st.markdown("### ğŸ“¡ Intelligence Feed")
                if not news_results:
                    st.warning("No news results found.")
                else:
                    for item in news_results:
                        with st.expander(f"{item['title']}", expanded=True):
                            st.write(item['body'])
                            st.markdown(f"[Source Article]({item['url']})")